# --- IQL specific parameters ---

# use epsilon greedy action selector
action_selector: "epsilon_greedy"
epsilon_start: 1.0
epsilon_finish: 0.05
epsilon_anneal_time: 5000 
evaluation_epsilon: 0.0
runner: "episode"

batch_size: 16 # 64 SANDBOX TESTING ------------------------------------
buffer_size: 100 # 2000 SANDBOX TESTING --------------------------------

t_max: 100000 # SANDBOX TESTING, DELETE AFTERWARDS ----------------------
global_reward: True # SANDBOX TESTING, DELETE AFTERWARDS ---------------

# update the target network every {} episodes
target_update_interval_or_tau: 200

mac: "ExpoComm_mac"
topk_neighbors: 7 # including itself
attention_dim: 16
agent: "ExpoComm_static_cont" # Default rnn agent
obs_agent_id: True
obs_last_action: True
obs_individual_obs: False

# use the Q_Learner to train

agent_output_type: "q"
learner: "cont_q_learner"
aux_coef: 0.1 # Coefficient for auxiliary loss
temperature: 0.07 # value from https://github.com/sthalles/SimCLR/blob/1848fc934ad844ae630e6c452300433fe99acfd9/run.py#L49
neg_num: 20
standardise_returns: False
standardise_rewards: True
double_q: True
mixer: "qmix"
use_rnn: True
mixing_embed_dim: 64
hypernet_layers: 2
hypernet_embed: 64

gamma: 0.95 
lr: 0.0001 # smaller learning rate

pos_flag: False

name: "qmix_ExpoComm_static_n7_cont"